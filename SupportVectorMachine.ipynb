{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import anderson\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and validation data\n",
    "train = pd.read_csv(\"/Users/jinwoolee/Downloads/Bank Project/insurance_t_var_sel.csv\")\n",
    "val = pd.read_csv(\"/Users/jinwoolee/Downloads/Bank Project/insurance_v_var_sel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCTAGE</th>\n",
       "      <th>DDA</th>\n",
       "      <th>DDABAL</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEPAMT</th>\n",
       "      <th>CHECKS</th>\n",
       "      <th>DIRDEP</th>\n",
       "      <th>NSF</th>\n",
       "      <th>NSFAMT</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>...</th>\n",
       "      <th>CCPURC</th>\n",
       "      <th>SDB</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>LORES</th>\n",
       "      <th>HMVAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>CRSCORE</th>\n",
       "      <th>INAREA</th>\n",
       "      <th>INS</th>\n",
       "      <th>BRANCH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1986.81</td>\n",
       "      <td>1</td>\n",
       "      <td>446.93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1069.78</td>\n",
       "      <td>5</td>\n",
       "      <td>6813.58</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>190.03</td>\n",
       "      <td>3</td>\n",
       "      <td>880.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>B14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>257.13</td>\n",
       "      <td>5</td>\n",
       "      <td>3408.35</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>B15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1072.55</td>\n",
       "      <td>1</td>\n",
       "      <td>590.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>111.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>998.25</td>\n",
       "      <td>2</td>\n",
       "      <td>1471.81</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>B18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACCTAGE  DDA   DDABAL  DEP   DEPAMT  CHECKS  DIRDEP  NSF  NSFAMT  PHONE  \\\n",
       "0      0.7    1  1986.81    1   446.93       1       1    0    0.00    0.0   \n",
       "1      4.1    0     0.00    0     0.00       0       0    0    0.00    0.0   \n",
       "2     12.3    1  1069.78    5  6813.58      13       1    0    0.00    2.0   \n",
       "3      0.8    1   190.03    3   880.25       1       0    1    5.65    NaN   \n",
       "4      1.6    1   257.13    5  3408.35      14       0    1   60.25    NaN   \n",
       "5      5.3    0     0.00    0     0.00       0       0    0    0.00    NaN   \n",
       "6      9.0    1  1072.55    1   590.25       0       0    1    5.78    NaN   \n",
       "7      4.0    0     0.00    0     0.00       0       0    0    0.00    NaN   \n",
       "8      NaN    1   998.25    2  1471.81       4       1    0    0.00    0.0   \n",
       "9      1.5    0     0.00    0     0.00       0       0    0    0.00    NaN   \n",
       "\n",
       "   ...  CCPURC  SDB  INCOME  LORES  HMVAL   AGE  CRSCORE  INAREA  INS  BRANCH  \n",
       "0  ...     1.0    0     4.0    7.0   87.0  51.0    674.0       1    0      B2  \n",
       "1  ...     0.0    0    30.0    8.5   97.0  60.0    640.0       1    1      B3  \n",
       "2  ...     0.0    0    19.0    3.0  107.0  55.0    662.0       1    1      B7  \n",
       "3  ...     NaN    1    20.0    4.0  107.0  40.0    642.0       1    0     B14  \n",
       "4  ...     NaN    0    24.0    8.5   95.0  54.0    732.0       1    0     B15  \n",
       "5  ...     NaN    0     8.0   12.5   76.0  61.0    688.0       1    1     B19  \n",
       "6  ...     NaN    0    45.0    8.5  111.0  54.0    635.0       1    1     B15  \n",
       "7  ...     NaN    0    35.0    3.5   99.0  46.0    642.0       1    1     B14  \n",
       "8  ...     0.0    0    88.0    8.0  129.0  22.0    626.0       1    1      B1  \n",
       "9  ...     NaN    0    31.0    4.0   97.0  53.0    693.0       0    0     B18  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First ten observations of train\n",
    "train.head(10)\n",
    "\n",
    "# We have a dataset derived from previous analysis, which allowed us to indentify \n",
    "# potential predictor variables related to the purchase of the annuity product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Null_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AGE</td>\n",
       "      <td>1702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INCOME</td>\n",
       "      <td>1537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LORES</td>\n",
       "      <td>1537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HMVAL</td>\n",
       "      <td>1537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PHONE</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POS</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POSAMT</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INV</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INVBAL</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CC</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CCBAL</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CCPURC</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCTAGE</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CRSCORE</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable  Null_Count\n",
       "12      AGE        1702\n",
       "9    INCOME        1537\n",
       "10    LORES        1537\n",
       "11    HMVAL        1537\n",
       "1     PHONE        1075\n",
       "2       POS        1075\n",
       "3    POSAMT        1075\n",
       "4       INV        1075\n",
       "5    INVBAL        1075\n",
       "6        CC        1075\n",
       "7     CCBAL        1075\n",
       "8    CCPURC        1075\n",
       "0   ACCTAGE         546\n",
       "13  CRSCORE         195"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find null values\n",
    "var_with_null = {'Variable': [], 'Null_Count': []}\n",
    "\n",
    "# Check all variables\n",
    "for column in train.columns:\n",
    "    null_count = train[column].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        var_with_null['Variable'].append(column)\n",
    "        var_with_null['Null_Count'].append(null_count)\n",
    "\n",
    "var_with_null = pd.DataFrame(var_with_null).sort_values(by = 'Null_Count', ascending = False)\n",
    "\n",
    "# Plot of the number of null values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x = 'Null_Count', y = 'Variable', data = var_with_null, color = 'navy')\n",
    "plt.title('Number of Null Values in Each Variable')\n",
    "plt.xlabel('Number of Null Values')\n",
    "plt.ylabel('Variable')\n",
    "plt.show()\n",
    "\n",
    "var_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCTAGE</th>\n",
       "      <th>DDA</th>\n",
       "      <th>DDABAL</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEPAMT</th>\n",
       "      <th>CHECKS</th>\n",
       "      <th>DIRDEP</th>\n",
       "      <th>NSF</th>\n",
       "      <th>NSFAMT</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>...</th>\n",
       "      <th>PHONE_FLAG</th>\n",
       "      <th>POS_FLAG</th>\n",
       "      <th>POSAMT_FLAG</th>\n",
       "      <th>INV_FLAG</th>\n",
       "      <th>INVBAL_FLAG</th>\n",
       "      <th>CC_FLAG</th>\n",
       "      <th>CCBAL_FLAG</th>\n",
       "      <th>CCPURC_FLAG</th>\n",
       "      <th>ACCTAGE_FLAG</th>\n",
       "      <th>CRSCORE_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1986.81</td>\n",
       "      <td>1</td>\n",
       "      <td>446.93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1069.78</td>\n",
       "      <td>5</td>\n",
       "      <td>6813.58</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>190.03</td>\n",
       "      <td>3</td>\n",
       "      <td>880.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>257.13</td>\n",
       "      <td>5</td>\n",
       "      <td>3408.35</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8490</th>\n",
       "      <td>10.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2262.87</td>\n",
       "      <td>2</td>\n",
       "      <td>4761.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8491</th>\n",
       "      <td>6.5</td>\n",
       "      <td>1</td>\n",
       "      <td>3683.22</td>\n",
       "      <td>2</td>\n",
       "      <td>3271.05</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8492</th>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>46593.63</td>\n",
       "      <td>3</td>\n",
       "      <td>55203.58</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8495 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ACCTAGE  DDA    DDABAL  DEP    DEPAMT  CHECKS  DIRDEP  NSF  NSFAMT  \\\n",
       "0         0.7    1   1986.81    1    446.93       1       1    0    0.00   \n",
       "1         4.1    0      0.00    0      0.00       0       0    0    0.00   \n",
       "2        12.3    1   1069.78    5   6813.58      13       1    0    0.00   \n",
       "3         0.8    1    190.03    3    880.25       1       0    1    5.65   \n",
       "4         1.6    1    257.13    5   3408.35      14       0    1   60.25   \n",
       "...       ...  ...       ...  ...       ...     ...     ...  ...     ...   \n",
       "8490     10.8    1   2262.87    2   4761.00       7       0    1    7.75   \n",
       "8491      6.5    1   3683.22    2   3271.05      13       1    0    0.00   \n",
       "8492      8.6    1  46593.63    3  55203.58       9       1    0    0.00   \n",
       "8493      NaN    0      0.00    0      0.00       0       0    0    0.00   \n",
       "8494     13.0    0      0.00    0      0.00       0       0    0    0.00   \n",
       "\n",
       "      PHONE  ...  PHONE_FLAG  POS_FLAG  POSAMT_FLAG  INV_FLAG  INVBAL_FLAG  \\\n",
       "0       0.0  ...           1         1            1         1            1   \n",
       "1       0.0  ...           1         1            1         1            1   \n",
       "2       2.0  ...           1         1            1         1            1   \n",
       "3       NaN  ...           0         0            0         0            0   \n",
       "4       NaN  ...           0         0            0         0            0   \n",
       "...     ...  ...         ...       ...          ...       ...          ...   \n",
       "8490    2.0  ...           1         1            1         1            1   \n",
       "8491    0.0  ...           1         1            1         1            1   \n",
       "8492    0.0  ...           1         1            1         1            1   \n",
       "8493    0.0  ...           1         1            1         1            1   \n",
       "8494    0.0  ...           1         1            1         1            1   \n",
       "\n",
       "      CC_FLAG  CCBAL_FLAG  CCPURC_FLAG  ACCTAGE_FLAG  CRSCORE_FLAG  \n",
       "0           1           1            1             1             1  \n",
       "1           1           1            1             1             1  \n",
       "2           1           1            1             1             1  \n",
       "3           0           0            0             1             1  \n",
       "4           0           0            0             1             1  \n",
       "...       ...         ...          ...           ...           ...  \n",
       "8490        1           1            1             1             1  \n",
       "8491        1           1            1             1             1  \n",
       "8492        1           1            1             1             1  \n",
       "8493        1           1            1             0             1  \n",
       "8494        1           1            1             1             1  \n",
       "\n",
       "[8495 rows x 52 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary flag variable that indicates missing values\n",
    "for var in var_with_null['Variable']:\n",
    "    train[f'{var}_FLAG'] = train[var].notna().astype(int)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify classes of variables: categorical or continuous\n",
    "cat_var = []\n",
    "cont_var = []\n",
    "\n",
    "for column in train.columns:\n",
    "    if column == 'BRANCH':\n",
    "        cat_var.append(column)\n",
    "    elif train[column].nunique() <= 10:\n",
    "        cat_var.append(column)\n",
    "    else:\n",
    "        cont_var.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use median and mode imputation for continuous and categorical variables, respectively\n",
    "for var in var_with_null['Variable']:\n",
    "    if var in cat_var:\n",
    "        train[var] = train[var].fillna(train[var].mode()[0])\n",
    "    else:\n",
    "        train[var] = train[var].fillna(train[var].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide predictor variables and target variable into individual dataframes\n",
    "X_train = train.drop(['INS'], axis = 1)\n",
    "Y_train = train['INS']\n",
    "\n",
    "# Create dummy variables for branch (branch is a categorical variable with string values)\n",
    "branch_dummy = pd.get_dummies((train['BRANCH']), drop_first=True)\n",
    "\n",
    "# Replace branch with the dummy variables created\n",
    "X_train = X_train.drop(['BRANCH'], axis = 1)\n",
    "X_train = pd.concat([X_train, branch_dummy], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "* If data follows a **normal** distribution, we can standardize it by using sklearn's StandardScaler\n",
    "    * Visualize the histogram\n",
    "    * **Shapiro-Wilk test** for small to moderate sized data\n",
    "    * **Anderson-Darling test** for data with kurtosis\n",
    "    * **Kolmogorov-Smirnov Test**\n",
    "    * These test all share the following:\n",
    "        * $H_{0}$: Normally distributed\n",
    "        * $H_{A}$: Not normally distributed\n",
    "* Otherwise (i.e., **skewed** or **binary**), apply midrange standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of predictor variables\n",
    "for column in X_train.columns:\n",
    "    plt.figure()  # Create a new figure for each histogram\n",
    "    X_train[column].plot.hist(title=column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# LORES, AGE, and CRSCORE may follow a normal distribution according to their histograms but need to conduct further test to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORES normality test results:\n",
      "KstestResult(statistic=0.9802529597477966, pvalue=0.0)\n",
      "AndersonResult(statistic=96.7676675691182, critical_values=array([0.576, 0.656, 0.787, 0.918, 1.091]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ])) \n",
      "\n",
      "AGE normality test results:\n",
      "KstestResult(statistic=1.0, pvalue=0.0)\n",
      "AndersonResult(statistic=78.57634386639256, critical_values=array([0.576, 0.656, 0.787, 0.918, 1.091]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ])) \n",
      "\n",
      "CRSCORE normality test results:\n",
      "KstestResult(statistic=1.0, pvalue=0.0)\n",
      "AndersonResult(statistic=1.3168096204572066, critical_values=array([0.576, 0.656, 0.787, 0.918, 1.091]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))\n"
     ]
    }
   ],
   "source": [
    "# Run the Anderson-Darling on LORES, AGE, and CRSCORE\n",
    "# If the returned statistic is larger than these critical values then for the corresponding significance level,\n",
    "# the null hypothesis that the data follows a normal distribution can be rejected. \n",
    "# Also run a KS Test on LORES, AGE, and CRSCORE\n",
    "print('LORES normality test results:')\n",
    "print(kstest(X_train['LORES'], 'norm'))\n",
    "print(anderson(X_train['LORES']), '\\n')\n",
    "print('AGE normality test results:')\n",
    "print(kstest(X_train['AGE'], 'norm'))\n",
    "print(anderson(X_train['AGE']), '\\n')\n",
    "print('CRSCORE normality test results:')\n",
    "print(kstest(X_train['CRSCORE'], 'norm'))\n",
    "print(anderson(X_train['CRSCORE']))\n",
    "\n",
    "# Given these results, for all variables in question, we have sufficient evidence to reject the null hypothesis\n",
    "# Hence, we need to apply midrange standardization for all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCTAGE</th>\n",
       "      <th>DDA</th>\n",
       "      <th>DDABAL</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEPAMT</th>\n",
       "      <th>CHECKS</th>\n",
       "      <th>DIRDEP</th>\n",
       "      <th>NSF</th>\n",
       "      <th>NSFAMT</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>...</th>\n",
       "      <th>B18</th>\n",
       "      <th>B19</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.975000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.984725</td>\n",
       "      <td>-0.928571</td>\n",
       "      <td>-0.998157</td>\n",
       "      <td>-0.959184</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.853571</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.560714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.991775</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>-0.971897</td>\n",
       "      <td>-0.469388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.971429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.998539</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.996369</td>\n",
       "      <td>-0.959184</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.964808</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.942857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.998023</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>-0.985942</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.624727</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8490</th>\n",
       "      <td>-0.614286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.982602</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.980363</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.951728</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8491</th>\n",
       "      <td>-0.767857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.971682</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.986508</td>\n",
       "      <td>-0.469388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8492</th>\n",
       "      <td>-0.692857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.641772</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.772306</td>\n",
       "      <td>-0.632653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8493</th>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>-0.535714</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8495 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACCTAGE  DDA    DDABAL       DEP    DEPAMT    CHECKS  DIRDEP  NSF  \\\n",
       "0    -0.975000  1.0 -0.984725 -0.928571 -0.998157 -0.959184     1.0 -1.0   \n",
       "1    -0.853571 -1.0 -1.000000 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "2    -0.560714  1.0 -0.991775 -0.642857 -0.971897 -0.469388     1.0 -1.0   \n",
       "3    -0.971429  1.0 -0.998539 -0.785714 -0.996369 -0.959184    -1.0  1.0   \n",
       "4    -0.942857  1.0 -0.998023 -0.642857 -0.985942 -0.428571    -1.0  1.0   \n",
       "...        ...  ...       ...       ...       ...       ...     ...  ...   \n",
       "8490 -0.614286  1.0 -0.982602 -0.857143 -0.980363 -0.714286    -1.0  1.0   \n",
       "8491 -0.767857  1.0 -0.971682 -0.857143 -0.986508 -0.469388     1.0 -1.0   \n",
       "8492 -0.692857  1.0 -0.641772 -0.785714 -0.772306 -0.632653     1.0 -1.0   \n",
       "8493 -0.857143 -1.0 -1.000000 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "8494 -0.535714 -1.0 -1.000000 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "\n",
       "        NSFAMT     PHONE  ...  B18  B19   B2   B3   B4   B5   B6   B7   B8  \\\n",
       "0    -1.000000 -1.000000  ... -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "1    -1.000000 -1.000000  ... -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "2    -1.000000 -0.733333  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0   \n",
       "3    -0.964808 -1.000000  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "4    -0.624727 -1.000000  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "...        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "8490 -0.951728 -0.733333  ... -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "8491 -1.000000 -1.000000  ... -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "8492 -1.000000 -1.000000  ... -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "8493 -1.000000 -1.000000  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "8494 -1.000000 -1.000000  ... -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0   \n",
       "\n",
       "       B9  \n",
       "0    -1.0  \n",
       "1    -1.0  \n",
       "2    -1.0  \n",
       "3    -1.0  \n",
       "4    -1.0  \n",
       "...   ...  \n",
       "8490 -1.0  \n",
       "8491 -1.0  \n",
       "8492 -1.0  \n",
       "8493 -1.0  \n",
       "8494 -1.0  \n",
       "\n",
       "[8495 rows x 68 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply midrange standardization on all variables\n",
    "X_train_scaled = X_train.copy()\n",
    "\n",
    "for predictor in X_train_scaled:\n",
    "    max = X_train_scaled[predictor].max()\n",
    "    min = X_train_scaled[predictor].min()\n",
    "    X_train_scaled[predictor] = (X_train_scaled[predictor] - ((max - min)/2)) / ((max - min)/2)\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "* **Feature Importance**: measures the average decrease in purity in the nodes of the tree (essentially measuring how much each variable improves the purity of the model when it is added)\n",
    "\n",
    "* **Hyperparameters examples**:\n",
    "\n",
    "    * **C** (Regularization parameter): C controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C leads to a larger margin but may allow more misclassifications on the training data, while a larger C emphasizes classifying the training data correctly but may result in a narrower margin and potential overfitting.\n",
    "\n",
    "    * Kernel choice: SVMs can use different kernel functions to map input data into higher-dimensional space, allowing for nonlinear decision boundaries. Common choices:\n",
    "\n",
    "        * **Linear** kernel: Suitable for linearly separable data or large datasets.\n",
    "        * **Polynomial** kernel: Allows modeling of non-linear decision boundaries. Parameters to tune include degree and coefficient of the polynomial.\n",
    "        * Radial Basis Function (**RBF**) kernel: Suitable for non-linearly separable data and widely used due to its flexibility. Parameters to tune include gamma.\n",
    "        \n",
    "    * **Gamma** for RBF kernel: Gamma determines the influence of each training example. A small gamma means a large radius for the Gaussian kernel, leading to a smoother decision \n",
    "    boundary, while a large gamma means a smaller radius, making the decision boundary more tightly around data points. The choice of gamma significantly impacts the model's complexity and generalization ability.\n",
    "\n",
    "    * **Degree** for Polynomial kernel: The degree parameter specifies the degree of the polynomial kernel function. Higher degrees can capture more complex relationships in the data but may also lead to overfitting.\n",
    "\n",
    "    * Class weights (**class_weight**): SVMs can be sensitive to class imbalance. The class_weight parameter allows you to assign different weights to different classes to balance their contribution to the model's training.\n",
    "\n",
    "    * Kernel coefficient (**coef0**): This parameter affects the influence of higher-order terms in the polynomial kernel function. It is used in polynomial and sigmoid kernels.\n",
    "\n",
    "    * Tolerance for stopping criterion (**tol**): The tolerance parameter determines the stopping criterion for the optimization process. Smaller values lead to longer training times but potentially better accuracy.\n",
    "    \n",
    "    * Probability estimation (**probability**): Setting probability=True enables probability estimates using Platt scaling or the more efficient method of isotonic regression. This can be useful if you need probability estimates rather than just class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's AUC is 0.76\n"
     ]
    }
   ],
   "source": [
    "# Build a Support Vector Machine model\n",
    "svm = SVC(random_state = 123, probability = True)\n",
    "svm.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Compute and add a column of predicted probabilities on the training dataset\n",
    "INS_hat = svm.predict_proba(X_train_scaled)[:,1]\n",
    "INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "X_train_scaled['INS_hat'] = INS_hat\n",
    "\n",
    "# Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, X_train_scaled['INS_hat'])\n",
    "\n",
    "# Print the training AUC\n",
    "print(\"Model's AUC is\", str(round(auc(fpr, tpr), 2)))\n",
    "X_train_scaled = X_train_scaled.drop(\"INS_hat\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   3.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   2.1s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   2.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   2.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   2.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   2.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.1s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.1s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   4.1s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   2.1s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   2.1s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   2.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   2.0s\n",
      "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   2.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.2s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   3.2s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   2.1s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   2.0s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   2.0s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   2.0s\n",
      "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   2.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.2s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   3.1s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   4.5s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   4.4s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   4.4s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   4.7s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   4.4s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.8s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.8s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.8s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.9s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   4.5s\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time=   4.5s\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time=   4.4s\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time=   4.4s\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time=   4.7s\n",
      "[CV] END .......................C=10, gamma=1, kernel=linear; total time=   4.4s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   3.6s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   3.5s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   3.5s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   3.5s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   3.5s\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   4.5s\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   4.5s\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   4.4s\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   4.7s\n",
      "[CV] END .....................C=10, gamma=0.1, kernel=linear; total time=   4.4s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   3.9s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   3.9s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  27.1s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  24.1s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  25.9s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  25.3s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  26.1s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.6s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.5s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.8s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.0s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  26.6s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  24.2s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  25.9s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  25.2s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  26.1s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   4.5s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   4.9s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.1s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   4.9s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   5.4s\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=  26.7s\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=  24.7s\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=  26.5s\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=  25.5s\n",
      "[CV] END ....................C=100, gamma=0.1, kernel=linear; total time=  26.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 'scale', 'kernel': 'linear'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First round of tuning hyperparameters\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': ['scale', 1, 0.1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svm, param_grid = param_grid, cv = 5, scoring = 'roc_auc', verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train_scaled, Y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's AUC is 0.78\n"
     ]
    }
   ],
   "source": [
    "# After first round of tuning\n",
    "svm = SVC(C = 100, gamma = 'scale', kernel = 'linear', random_state = 123, probability = True)\n",
    "svm.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Compute and add a column of predicted probabilities on the training dataset\n",
    "INS_hat = svm.predict_proba(X_train_scaled)[:,1]\n",
    "INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "X_train_scaled['INS_hat'] = INS_hat\n",
    "\n",
    "# Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, X_train_scaled['INS_hat'])\n",
    "\n",
    "# Print the training AUC\n",
    "base_auc = auc(fpr, tpr)\n",
    "print(\"Model's AUC is\", str(round(auc(fpr, tpr), 2)))\n",
    "X_train_scaled = X_train_scaled.drop(\"INS_hat\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MM</th>\n",
       "      <td>0.287360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAVBAL</th>\n",
       "      <td>0.186001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DDABAL</th>\n",
       "      <td>0.110733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>0.077562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMBAL</th>\n",
       "      <td>0.064578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HMVAL</th>\n",
       "      <td>-0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B6</th>\n",
       "      <td>-0.000629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POSAMT_FLAG</th>\n",
       "      <td>-0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC_FLAG</th>\n",
       "      <td>-0.001197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCBAL</th>\n",
       "      <td>-0.001811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Importance\n",
       "MM             0.287360\n",
       "SAVBAL         0.186001\n",
       "DDABAL         0.110733\n",
       "CD             0.077562\n",
       "MMBAL          0.064578\n",
       "...                 ...\n",
       "HMVAL         -0.000410\n",
       "B6            -0.000629\n",
       "POSAMT_FLAG   -0.000695\n",
       "CC_FLAG       -0.001197\n",
       "CCBAL         -0.001811\n",
       "\n",
       "[68 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty dataframe to store feature importance\n",
    "importances = pd.DataFrame(index =  X_train_scaled.columns)\n",
    "\n",
    "# Iterate through each feature and computed the the difference between permuted and baseline AUC\n",
    "for predictor in X_train_scaled.columns:\n",
    "\n",
    "    # Permutate each predictor variable\n",
    "    predictor_permuted = copy.deepcopy(X_train_scaled)\n",
    "    predictor_permuted[predictor] = np.random.permutation(predictor_permuted[predictor])\n",
    "\n",
    "    # Compute and add a column of predicted probabilities on the training dataset\n",
    "    INS_hat = svm.predict_proba(predictor_permuted)[:,1]\n",
    "    INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "    predictor_permuted['INS_hat'] = INS_hat\n",
    "\n",
    "    # Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "    fpr, tpr, thresholds = roc_curve(Y_train, predictor_permuted['INS_hat'])\n",
    "    auc_after_permutation = auc(fpr, tpr)\n",
    "\n",
    "    # Difference between base AUC and AUC after permutation\n",
    "    importances.loc[predictor, 'Importance'] = base_auc - auc_after_permutation\n",
    "    predictor_permuted = predictor_permuted.drop(\"INS_hat\", axis = 1)\n",
    "\n",
    "# Normalize importances\n",
    "importances['Importance'] = importances['Importance']/importances['Importance'].sum()\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the variables with negative importance\n",
    "neg_importance = importances[importances['Importance'] > 0]\n",
    "\n",
    "X_train_scaled_reduced = copy.deepcopy(X_train_scaled)\n",
    "X_train_scaled_reduced = X_train_scaled[neg_importance.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's AUC is 0.78\n"
     ]
    }
   ],
   "source": [
    "# See how the model performs with reduced number of features\n",
    "svm = SVC(C = 100, gamma = 'scale', kernel = 'linear', random_state = 123, probability = True)\n",
    "svm.fit(X_train_scaled_reduced, Y_train)\n",
    "\n",
    "# Compute and add a column of predicted probabilities on the training dataset\n",
    "INS_hat = svm.predict_proba(X_train_scaled_reduced)[:,1]\n",
    "INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "X_train_scaled_reduced['INS_hat'] = INS_hat\n",
    "\n",
    "# Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, X_train_scaled_reduced['INS_hat'])\n",
    "\n",
    "# Print the training AUC\n",
    "print(\"Model's AUC is\", str(round(auc(fpr, tpr), 2)))\n",
    "\n",
    "# The model with reduced number of predictors does not have a higher AUC\n",
    "# We move forward with the dataset with all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END ......................C=75, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END ......................C=75, gamma=scale, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=75, gamma=scale, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=75, gamma=scale, kernel=rbf; total time=   3.7s\n",
      "[CV] END ......................C=75, gamma=scale, kernel=rbf; total time=   3.8s\n",
      "[CV] END ...................C=75, gamma=scale, kernel=linear; total time=  23.4s\n",
      "[CV] END ...................C=75, gamma=scale, kernel=linear; total time=  19.9s\n",
      "[CV] END ...................C=75, gamma=scale, kernel=linear; total time=  20.6s\n",
      "[CV] END ...................C=75, gamma=scale, kernel=linear; total time=  20.1s\n",
      "[CV] END ...................C=75, gamma=scale, kernel=linear; total time=  20.2s\n",
      "[CV] END ..........................C=75, gamma=1, kernel=rbf; total time=   4.8s\n",
      "[CV] END ..........................C=75, gamma=1, kernel=rbf; total time=   5.2s\n",
      "[CV] END ..........................C=75, gamma=1, kernel=rbf; total time=   5.3s\n",
      "[CV] END ..........................C=75, gamma=1, kernel=rbf; total time=   4.8s\n",
      "[CV] END ..........................C=75, gamma=1, kernel=rbf; total time=   5.8s\n",
      "[CV] END .......................C=75, gamma=1, kernel=linear; total time=  23.9s\n",
      "[CV] END .......................C=75, gamma=1, kernel=linear; total time=  20.1s\n",
      "[CV] END .......................C=75, gamma=1, kernel=linear; total time=  21.1s\n",
      "[CV] END .......................C=75, gamma=1, kernel=linear; total time=  20.6s\n",
      "[CV] END .......................C=75, gamma=1, kernel=linear; total time=  20.5s\n",
      "[CV] END .......................C=75, gamma=0.01, kernel=rbf; total time=   3.7s\n",
      "[CV] END .......................C=75, gamma=0.01, kernel=rbf; total time=   3.7s\n",
      "[CV] END .......................C=75, gamma=0.01, kernel=rbf; total time=   3.7s\n",
      "[CV] END .......................C=75, gamma=0.01, kernel=rbf; total time=   3.6s\n",
      "[CV] END .......................C=75, gamma=0.01, kernel=rbf; total time=   3.6s\n",
      "[CV] END ....................C=75, gamma=0.01, kernel=linear; total time=  24.0s\n",
      "[CV] END ....................C=75, gamma=0.01, kernel=linear; total time=  20.4s\n",
      "[CV] END ....................C=75, gamma=0.01, kernel=linear; total time=  20.7s\n",
      "[CV] END ....................C=75, gamma=0.01, kernel=linear; total time=  20.4s\n",
      "[CV] END ....................C=75, gamma=0.01, kernel=linear; total time=  20.7s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.1s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.1s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   4.2s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  27.3s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  24.4s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  26.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  25.1s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=  26.3s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.9s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   4.5s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   5.1s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  27.1s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  24.7s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  26.3s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  26.0s\n",
      "[CV] END ......................C=100, gamma=1, kernel=linear; total time=  26.9s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   3.9s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   3.8s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   3.8s\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=  27.7s\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=  24.9s\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=  25.9s\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=  25.2s\n",
      "[CV] END ...................C=100, gamma=0.01, kernel=linear; total time=  26.0s\n",
      "[CV] END .....................C=200, gamma=scale, kernel=rbf; total time=   4.7s\n",
      "[CV] END .....................C=200, gamma=scale, kernel=rbf; total time=   4.8s\n",
      "[CV] END .....................C=200, gamma=scale, kernel=rbf; total time=   4.8s\n",
      "[CV] END .....................C=200, gamma=scale, kernel=rbf; total time=   4.6s\n",
      "[CV] END .....................C=200, gamma=scale, kernel=rbf; total time=   4.8s\n",
      "[CV] END ..................C=200, gamma=scale, kernel=linear; total time=  53.5s\n",
      "[CV] END ..................C=200, gamma=scale, kernel=linear; total time=  57.8s\n",
      "[CV] END ..................C=200, gamma=scale, kernel=linear; total time=  48.6s\n",
      "[CV] END ..................C=200, gamma=scale, kernel=linear; total time=  47.1s\n",
      "[CV] END ..................C=200, gamma=scale, kernel=linear; total time=  50.3s\n",
      "[CV] END .........................C=200, gamma=1, kernel=rbf; total time=   5.1s\n",
      "[CV] END .........................C=200, gamma=1, kernel=rbf; total time=   4.6s\n",
      "[CV] END .........................C=200, gamma=1, kernel=rbf; total time=   5.2s\n",
      "[CV] END .........................C=200, gamma=1, kernel=rbf; total time=   5.1s\n",
      "[CV] END .........................C=200, gamma=1, kernel=rbf; total time=   5.7s\n",
      "[CV] END ......................C=200, gamma=1, kernel=linear; total time=  54.2s\n",
      "[CV] END ......................C=200, gamma=1, kernel=linear; total time=  58.8s\n",
      "[CV] END ......................C=200, gamma=1, kernel=linear; total time=  49.8s\n",
      "[CV] END ......................C=200, gamma=1, kernel=linear; total time=  47.7s\n",
      "[CV] END ......................C=200, gamma=1, kernel=linear; total time=  51.0s\n",
      "[CV] END ......................C=200, gamma=0.01, kernel=rbf; total time=   4.6s\n",
      "[CV] END ......................C=200, gamma=0.01, kernel=rbf; total time=   4.5s\n",
      "[CV] END ......................C=200, gamma=0.01, kernel=rbf; total time=   4.4s\n",
      "[CV] END ......................C=200, gamma=0.01, kernel=rbf; total time=   4.3s\n",
      "[CV] END ......................C=200, gamma=0.01, kernel=rbf; total time=   4.4s\n",
      "[CV] END ...................C=200, gamma=0.01, kernel=linear; total time=  54.6s\n",
      "[CV] END ...................C=200, gamma=0.01, kernel=linear; total time= 1.0min\n",
      "[CV] END ...................C=200, gamma=0.01, kernel=linear; total time=  58.1s\n",
      "[CV] END ...................C=200, gamma=0.01, kernel=linear; total time=  59.3s\n",
      "[CV] END ...................C=200, gamma=0.01, kernel=linear; total time= 1.1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 200, 'gamma': 'scale', 'kernel': 'linear'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second round of tuning hyperparameters\n",
    "param_grid = {\n",
    "    'C': [75, 100, 200],\n",
    "    'gamma': ['scale', 1, 0.01],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svm, param_grid = param_grid, cv = 5, scoring = 'roc_auc', verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train_scaled, Y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's AUC is 0.78\n"
     ]
    }
   ],
   "source": [
    "# After second round of tuning\n",
    "svm = SVC(C = 200, gamma = 'scale', kernel = 'linear', random_state = 123, probability = True)\n",
    "svm.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Compute and add a column of predicted probabilities on the training dataset\n",
    "INS_hat = svm.predict_proba(X_train_scaled)[:,1]\n",
    "INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "X_train_scaled['INS_hat'] = INS_hat\n",
    "\n",
    "# Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, X_train_scaled['INS_hat'])\n",
    "\n",
    "# Print the training AUC\n",
    "base_auc = auc(fpr, tpr)\n",
    "print(\"Model's AUC is\", str(round(auc(fpr, tpr), 2)))\n",
    "X_train_scaled = X_train_scaled.drop(\"INS_hat\", axis = 1)\n",
    "\n",
    "# Second round of tuning did not improve AUC and adds a layer of unnecessary complication\n",
    "# Thus, we move forward with the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, kernel=&#x27;linear&#x27;, probability=True, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=100, kernel=&#x27;linear&#x27;, probability=True, random_state=123)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, kernel='linear', probability=True, random_state=123)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final model\n",
    "svm = SVC(C = 100, gamma = 'scale', kernel = 'linear', random_state = 123, probability = True)\n",
    "svm.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities on the training dataset\n",
    "INS_hat = svm.predict_proba(X_train_scaled)[:,1]\n",
    "INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "\n",
    "X_train_scaled['INS_hat'] = INS_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Cut-off</th>\n",
       "      <th>Youden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>0.778958</td>\n",
       "      <td>0.322754</td>\n",
       "      <td>0.270388</td>\n",
       "      <td>0.456204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>0.779986</td>\n",
       "      <td>0.323830</td>\n",
       "      <td>0.270174</td>\n",
       "      <td>0.456156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.318809</td>\n",
       "      <td>0.271891</td>\n",
       "      <td>0.456036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>0.775188</td>\n",
       "      <td>0.319168</td>\n",
       "      <td>0.271721</td>\n",
       "      <td>0.456020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>0.779301</td>\n",
       "      <td>0.323471</td>\n",
       "      <td>0.270314</td>\n",
       "      <td>0.455829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TPR       FPR   Cut-off    Youden\n",
       "1857  0.778958  0.322754  0.270388  0.456204\n",
       "1861  0.779986  0.323830  0.270174  0.456156\n",
       "1839  0.774846  0.318809  0.271891  0.456036\n",
       "1841  0.775188  0.319168  0.271721  0.456020\n",
       "1859  0.779301  0.323471  0.270314  0.455829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal cutoff is 0.27\n",
      "Youden's Index (K-S Statistic) is 0.46\n",
      "Model's AUC is 0.78\n"
     ]
    }
   ],
   "source": [
    "# Visualize the ROC curve\n",
    "\n",
    "# Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, X_train_scaled['INS_hat'])\n",
    "\n",
    "# Compute Youden's Index\n",
    "data = {'TPR': tpr, 'FPR': fpr, 'Cut-off': thresholds, 'Youden': tpr-fpr}\n",
    "youden = pd.DataFrame(data)\n",
    "youden = youden.sort_values(by = ['Youden'], ascending = False)\n",
    "display(youden.head(5))\n",
    "\n",
    "# Plot ROC curve\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc = auc(fpr, tpr))\n",
    "roc_display.plot()\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('1-Specificity (FPR)')\n",
    "plt.ylabel('Sensitivity (TPR)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal cutoff, Youden's index, and AUC\n",
    "print(\"Optimal cutoff is\", round(youden.loc[1857, 'Cut-off'], 2))\n",
    "print(\"Youden's Index (K-S Statistic) is\", round(youden.loc[1857, 'Youden'], 2))\n",
    "print(\"Model's AUC is\", str(round(auc(fpr, tpr), 2)))\n",
    "\n",
    "# Our lowest AUC on the training dataset so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset\n",
    "* Apply all changes made to the training dataset onto the validation dataset\n",
    "* Get the predicted probabilities of all observations and compute the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCTAGE</th>\n",
       "      <th>DDA</th>\n",
       "      <th>DDABAL</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEPAMT</th>\n",
       "      <th>CHECKS</th>\n",
       "      <th>DIRDEP</th>\n",
       "      <th>NSF</th>\n",
       "      <th>NSFAMT</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>...</th>\n",
       "      <th>B18</th>\n",
       "      <th>B19</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.960352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.990533</td>\n",
       "      <td>-0.764706</td>\n",
       "      <td>-0.974538</td>\n",
       "      <td>-0.794872</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.986784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.883618</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.801762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.965986</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-0.892647</td>\n",
       "      <td>-0.692308</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.876652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.978729</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-0.946841</td>\n",
       "      <td>-0.794872</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.726872</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>-0.925110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.983848</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>-0.883478</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.767571</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>-0.938326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.998302</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>-0.951542</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.852082</td>\n",
       "      <td>-0.647059</td>\n",
       "      <td>-0.932978</td>\n",
       "      <td>-0.948718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>-0.876652</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>-0.828194</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2124 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ACCTAGE  DDA    DDABAL       DEP    DEPAMT    CHECKS  DIRDEP  NSF  \\\n",
       "0    -0.960352  1.0 -0.990533 -0.764706 -0.974538 -0.794872    -1.0 -1.0   \n",
       "1    -0.986784  1.0 -0.883618 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "2    -0.801762  1.0 -0.965986 -0.529412 -0.892647 -0.692308    -1.0 -1.0   \n",
       "3    -0.876652  1.0 -0.978729 -0.647059 -0.946841 -0.794872    -1.0 -1.0   \n",
       "4    -0.726872 -1.0 -1.000000 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "...        ...  ...       ...       ...       ...       ...     ...  ...   \n",
       "2119 -0.925110  1.0 -0.983848 -0.294118 -0.883478 -0.230769     1.0  1.0   \n",
       "2120 -0.938326  1.0 -0.998302 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "2121 -0.951542  1.0 -0.852082 -0.647059 -0.932978 -0.948718     1.0 -1.0   \n",
       "2122 -0.876652 -1.0 -1.000000 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "2123 -0.828194 -1.0 -1.000000 -1.000000 -1.000000 -1.000000    -1.0 -1.0   \n",
       "\n",
       "        NSFAMT  PHONE  ...  B18  B19   B2   B3   B4   B5   B6   B7   B8   B9  \n",
       "0    -1.000000   -1.0  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  \n",
       "1    -1.000000   -1.0  ... -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "2    -1.000000   -0.8  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "3    -1.000000   -1.0  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "4    -1.000000   -1.0  ... -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "...        ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2119 -0.767571   -1.0  ... -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "2120 -1.000000   -1.0  ... -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0  \n",
       "2121 -1.000000   -1.0  ... -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "2122 -1.000000   -1.0  ... -1.0 -1.0 -1.0 -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "2123 -1.000000   -1.0  ... -1.0 -1.0  1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "\n",
       "[2124 rows x 68 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary flag variable that indicates missing values\n",
    "for var in var_with_null['Variable']:\n",
    "    val[f'{var}_FLAG'] = val[var].notna().astype(int)\n",
    "\n",
    "# Use median and mode imputation for continuous and categorical variables, respectively\n",
    "for var in var_with_null['Variable']:\n",
    "    if var in cat_var:\n",
    "        val[var] = val[var].fillna(train[var].mode()[0])\n",
    "    else:\n",
    "        val[var] = val[var].fillna(train[var].median())\n",
    "\n",
    "# Divide predictor variables and target variable into individual dataframes\n",
    "X_val = val.drop(['INS'], axis = 1)\n",
    "Y_val = val['INS']\n",
    "\n",
    "# Create dummy variables for branch (branch is a categorical variable with string values)\n",
    "branch_dummy = pd.get_dummies((val['BRANCH']), drop_first=True)\n",
    "\n",
    "# Replace branch with the dummy variables created\n",
    "X_val = X_val.drop(['BRANCH'], axis = 1)\n",
    "X_val = pd.concat([X_val, branch_dummy], axis = 1)\n",
    "\n",
    "# Apply midrange standardization on all variables\n",
    "X_val_scaled = X_val.copy()\n",
    "\n",
    "for predictor in X_val_scaled:\n",
    "    max = X_val_scaled[predictor].max()\n",
    "    min = X_val_scaled[predictor].min()\n",
    "    X_val_scaled[predictor] = (X_val_scaled[predictor] - ((max - min)/2)) / ((max - min)/2)\n",
    "\n",
    "X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and add a column of predicted probabilities on the validation dataset\n",
    "INS_hat = svm.predict_proba(X_val_scaled)[:,1]\n",
    "INS_hat = pd.DataFrame({'INS_hat': INS_hat})\n",
    "X_val_scaled['INS_hat'] = INS_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's AUC is 0.78\n"
     ]
    }
   ],
   "source": [
    "# Visualize the ROC curve\n",
    "\n",
    "# Establish the false positive rate, true positive rate, and the optimal cutoff\n",
    "fpr, tpr, thresholds = roc_curve(Y_val, X_val_scaled['INS_hat'])\n",
    "\n",
    "# Plot ROC curve\n",
    "roc_display = RocCurveDisplay(fpr = fpr, tpr = tpr, roc_auc = auc(fpr, tpr))\n",
    "roc_display.plot()\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('1-Specificity (FPR)')\n",
    "plt.ylabel('Sensitivity (TPR)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC\n",
    "print(\"Model's AUC is\", str(round(auc(fpr, tpr), 2)))\n",
    "\n",
    "# Performed relatively similarly compared to the training dataset which also had an AUC of 0.78\n",
    "# However, the SVM had the same AUC as the logistic regression model at 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>INS_pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>624</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "INS_pred    0    1\n",
       "INS               \n",
       "0         624  758\n",
       "1          59  683"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix where we make predictions from predicted probabilities based on the optimal cut-off\n",
    "X_val_scaled['INS_pred'] = X_val_scaled['INS_hat'].map(lambda x: 1 if x > (youden.loc[1857, 'Cut-off']) else 0)\n",
    "pd.crosstab(Y_val, X_val_scaled['INS_pred'])\n",
    "\n",
    "# Although the AUC was the lowest, notice that out of all the models, this model did the best when it came to capturing the actual purchases (due to a lower cut-off)\n",
    "# High sensitivity would be EXTREMELY essential in cases where even capturing marginally more purchases would tremendously increase profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_train = X_train_scaled[['INS_hat']]\n",
    "svm_results_train.to_csv('Train_Results_SVM.csv')\n",
    "\n",
    "svm_results_val = X_val_scaled[['INS_hat']]\n",
    "svm_results_val.to_csv('Val_Results_SVM.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
